{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries, load and transform data","metadata":{}},{"cell_type":"code","source":"# Install necessary Python packages using pip\n\n# Use the 'pip' command to install packages\n# The '-q' flag stands for 'quiet,' which means it will suppress most output, making the installation process less verbose\n# We're installing the following packages:\n# - 'evaluate': This package is likely used for evaluation purposes, but the specific functionality is not clear from this line alone\n# - 'transformers': This package is commonly used for natural language processing tasks, such as working with pre-trained language models like BERT or GPT\n# - 'datasets': This package provides easy access to various datasets commonly used in machine learning and natural language processing tasks\n# - 'mlflow': MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models\n\n# Note: Before running this code, make sure you have Python and pip installed on your system.\n# Also, ensure you have an internet connection since pip will download and install these packages from PyPI (Python Package Index).\n!pip install -U -q evaluate transformers datasets>=2.14.5 mlflow 2>/dev/null","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:30:48.458468Z","iopub.execute_input":"2023-10-22T17:30:48.458815Z","iopub.status.idle":"2023-10-22T17:31:15.081308Z","shell.execute_reply.started":"2023-10-22T17:30:48.458787Z","shell.execute_reply":"2023-10-22T17:31:15.080084Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries and modules\nimport warnings  # Import the 'warnings' module for handling warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore warnings during execution\n\nimport gc  # Import the 'gc' module for garbage collection\nimport numpy as np  # Import NumPy for numerical operations\nimport pandas as pd  # Import Pandas for data manipulation\nimport itertools  # Import 'itertools' for iterators and looping\nfrom collections import Counter  # Import 'Counter' for counting elements\nimport matplotlib.pyplot as plt  # Import Matplotlib for data visualization\nfrom sklearn.metrics import (  # Import various metrics from scikit-learn\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,  # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\n# Import custom modules and classes\nfrom imblearn.over_sampling import RandomOverSampler # import RandomOverSampler\nimport evaluate  # Import the 'evaluate' module\nfrom datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\nfrom transformers import (  # Import various modules from the Transformers library\n    TrainingArguments,  # For training arguments\n    Trainer,  # For model training\n    ViTImageProcessor,  # For processing image data with ViT models\n    ViTForImageClassification,  # ViT model for image classification\n    DefaultDataCollator  # For collating data in the default way\n)\nimport torch  # Import PyTorch for deep learning\nfrom torch.utils.data import DataLoader  # For creating data loaders\nfrom torchvision.transforms import (  # Import image transformation functions\n    CenterCrop,  # Center crop an image\n    Compose,  # Compose multiple image transformations\n    Normalize,  # Normalize image pixel values\n    RandomRotation,  # Apply random rotation to images\n    RandomResizedCrop,  # Crop and resize images randomly\n    RandomHorizontalFlip,  # Apply random horizontal flip\n    RandomAdjustSharpness,  # Adjust sharpness randomly\n    Resize,  # Resize images\n    ToTensor  # Convert images to PyTorch tensors\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:31:15.083626Z","iopub.execute_input":"2023-10-22T17:31:15.084014Z","iopub.status.idle":"2023-10-22T17:31:31.463539Z","shell.execute_reply.started":"2023-10-22T17:31:15.083975Z","shell.execute_reply":"2023-10-22T17:31:31.462569Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Import the necessary module from the Python Imaging Library (PIL).\nfrom PIL import ImageFile\n\n# Enable the option to load truncated images.\n# This setting allows the PIL library to attempt loading images even if they are corrupted or incomplete.\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:31:31.464823Z","iopub.execute_input":"2023-10-22T17:31:31.465178Z","iopub.status.idle":"2023-10-22T17:31:31.469892Z","shell.execute_reply.started":"2023-10-22T17:31:31.465146Z","shell.execute_reply":"2023-10-22T17:31:31.468960Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# use https://huggingface.co/docs/datasets/image_load for reference\n\n# Import necessary libraries\nimage_dict = {}\n\n# Define the list of file names\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport os\n# Initialize empty lists to store file names and labels\nfile_names = []\nlabels = []\n\n# Iterate through all image files in the specified directory\nfor file in tqdm(sorted((Path('/kaggle/input/drivers-drowsiness-detection/Final/PreparedData/').glob('*/*/*.*')))):\n    label = str(file).split('/')[-2]  # Extract the label from the file path\n    labels.append(label)  # Add the label to the list\n    file_names.append(str(file))  # Add the file path to the list\n\n# Print the total number of file names and labels\nprint(len(file_names), len(labels))\n\n# Create a pandas dataframe from the collected file names and labels\ndf = pd.DataFrame.from_dict({\"image\": file_names, \"label\": labels})\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:32:53.191615Z","iopub.execute_input":"2023-10-22T17:32:53.191942Z","iopub.status.idle":"2023-10-22T17:32:56.306489Z","shell.execute_reply.started":"2023-10-22T17:32:53.191917Z","shell.execute_reply":"2023-10-22T17:32:56.305578Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 84898/84898 [00:00<00:00, 221912.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"84898 84898\n(84898, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:39.793526Z","iopub.execute_input":"2023-10-22T17:33:39.794340Z","iopub.status.idle":"2023-10-22T17:33:39.805347Z","shell.execute_reply.started":"2023-10-22T17:33:39.794279Z","shell.execute_reply":"2023-10-22T17:33:39.804332Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                               image     label\n0  /kaggle/input/drivers-drowsiness-detection/Fin...  closeEye\n1  /kaggle/input/drivers-drowsiness-detection/Fin...  closeEye\n2  /kaggle/input/drivers-drowsiness-detection/Fin...  closeEye\n3  /kaggle/input/drivers-drowsiness-detection/Fin...  closeEye\n4  /kaggle/input/drivers-drowsiness-detection/Fin...  closeEye","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/drivers-drowsiness-detection/Fin...</td>\n      <td>closeEye</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/drivers-drowsiness-detection/Fin...</td>\n      <td>closeEye</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/drivers-drowsiness-detection/Fin...</td>\n      <td>closeEye</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/drivers-drowsiness-detection/Fin...</td>\n      <td>closeEye</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/drivers-drowsiness-detection/Fin...</td>\n      <td>closeEye</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:40.843417Z","iopub.execute_input":"2023-10-22T17:33:40.843922Z","iopub.status.idle":"2023-10-22T17:33:40.861341Z","shell.execute_reply.started":"2023-10-22T17:33:40.843884Z","shell.execute_reply":"2023-10-22T17:33:40.860340Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array(['closeEye', 'openEye'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"# random oversampling of minority class\n# 'y' contains the target variable (label) we want to predict\ny = df[['label']]\n\n# Drop the 'label' column from the DataFrame 'df' to separate features from the target variable\ndf = df.drop(['label'], axis=1)\n\n# Create a RandomOverSampler object with a specified random seed (random_state=83)\nros = RandomOverSampler(random_state=83)\n\n# Use the RandomOverSampler to resample the dataset by oversampling the minority class\n# 'df' contains the feature data, and 'y_resampled' will contain the resampled target variable\ndf, y_resampled = ros.fit_resample(df, y)\n\n# Delete the original 'y' variable to save memory as it's no longer needed\ndel y\n\n# Add the resampled target variable 'y_resampled' as a new 'label' column in the DataFrame 'df'\ndf['label'] = y_resampled\n\n# Delete the 'y_resampled' variable to save memory as it's no longer needed\ndel y_resampled\n\n# Perform garbage collection to free up memory used by discarded variables\ngc.collect()\n\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:41.603476Z","iopub.execute_input":"2023-10-22T17:33:41.603833Z","iopub.status.idle":"2023-10-22T17:33:42.387476Z","shell.execute_reply.started":"2023-10-22T17:33:41.603805Z","shell.execute_reply":"2023-10-22T17:33:42.386515Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(85904, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a dataset from a Pandas DataFrame.\ndataset = Dataset.from_pandas(df).cast_column(\"image\", Image())","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:42.633554Z","iopub.execute_input":"2023-10-22T17:33:42.634091Z","iopub.status.idle":"2023-10-22T17:33:42.718673Z","shell.execute_reply.started":"2023-10-22T17:33:42.634060Z","shell.execute_reply":"2023-10-22T17:33:42.717945Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Display the first image in the dataset\ndataset[0][\"image\"]","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:44.266643Z","iopub.execute_input":"2023-10-22T17:33:44.267362Z","iopub.status.idle":"2023-10-22T17:33:44.287283Z","shell.execute_reply.started":"2023-10-22T17:33:44.267328Z","shell.execute_reply":"2023-10-22T17:33:44.286472Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<PIL.PngImagePlugin.PngImageFile image mode=L size=86x86>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# Extracting a subset of elements from the 'labels' list using slicing.\n# The slicing syntax [:5] selects elements from the beginning up to (but not including) the 5th element.\n# This will give us the first 5 elements of the 'labels' list.\n# The result will be a new list containing these elements.\nlabels_subset = labels[:5]\n\n# Printing the subset of labels to inspect the content.\nprint(labels_subset)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:44.773817Z","iopub.execute_input":"2023-10-22T17:33:44.774123Z","iopub.status.idle":"2023-10-22T17:33:44.779272Z","shell.execute_reply.started":"2023-10-22T17:33:44.774099Z","shell.execute_reply":"2023-10-22T17:33:44.778420Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"['closeEye', 'closeEye', 'closeEye', 'closeEye', 'closeEye']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a list of unique labels by converting 'labels' to a set and then back to a list\nlabels_list = list(set(labels))\n\n# Initialize empty dictionaries to map labels to IDs and vice versa\nlabel2id, id2label = dict(), dict()\n\n# Iterate over the unique labels and assign each label an ID, and vice versa\nfor i, label in enumerate(labels_list):\n    label2id[label] = i  # Map the label to its corresponding ID\n    id2label[i] = label  # Map the ID to its corresponding label\n\n# Print the resulting dictionaries for reference\nprint(\"Mapping of IDs to Labels:\", id2label, '\\n')\nprint(\"Mapping of Labels to IDs:\", label2id)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:45.756944Z","iopub.execute_input":"2023-10-22T17:33:45.757631Z","iopub.status.idle":"2023-10-22T17:33:45.767771Z","shell.execute_reply.started":"2023-10-22T17:33:45.757596Z","shell.execute_reply":"2023-10-22T17:33:45.766814Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Mapping of IDs to Labels: {0: 'closeEye', 1: 'openEye'} \n\nMapping of Labels to IDs: {'closeEye': 0, 'openEye': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating classlabels to match labels to IDs\nClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n\n# Mapping labels to IDs\ndef map_label2id(example):\n    example['label'] = ClassLabels.str2int(example['label'])\n    return example\n\ndataset = dataset.map(map_label2id, batched=True)\n\n# Casting label column to ClassLabel Object\ndataset = dataset.cast_column('label', ClassLabels)\n\n# Splitting the dataset into training and testing sets using an 80-20 split ratio.\ndataset = dataset.train_test_split(test_size=0.2, shuffle=True, stratify_by_column=\"label\")\n\n# Extracting the training data from the split dataset.\ntrain_data = dataset['train']\n\n# Extracting the testing data from the split dataset.\ntest_data = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:46.613708Z","iopub.execute_input":"2023-10-22T17:33:46.614059Z","iopub.status.idle":"2023-10-22T17:33:47.041491Z","shell.execute_reply.started":"2023-10-22T17:33:46.614029Z","shell.execute_reply":"2023-10-22T17:33:47.040733Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/85904 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5bc9757f87f476e94efed5cdd298a00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/85904 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4926bfc65b34338b582ee4c4f198224"}},"metadata":{}}]},{"cell_type":"code","source":"# Define the pre-trained ViT model string\nmodel_str = 'google/vit-base-patch16-224-in21k'\n\n# Create a processor for ViT model input from the pre-trained model\nprocessor = ViTImageProcessor.from_pretrained(model_str)\n\n# Retrieve the image mean and standard deviation used for normalization\nimage_mean, image_std = processor.image_mean, processor.image_std\n\n# Get the size (height) of the ViT model's input images\nsize = processor.size[\"height\"]\nprint(\"Size: \", size)\n\n# Define a normalization transformation for the input images\nnormalize = Normalize(mean=image_mean, std=image_std)\n\n# Define a set of transformations for training data\n_train_transforms = Compose(\n    [\n        Resize((size, size)),             # Resize images to the ViT model's input size\n        RandomRotation(30),               # Apply random rotation\n        RandomAdjustSharpness(2),         # Adjust sharpness randomly\n        ToTensor(),                       # Convert images to tensors\n        normalize                         # Normalize images using mean and std\n    ]\n)\n\n# Define a set of transformations for validation data\n_val_transforms = Compose(\n    [\n        Resize((size, size)),             # Resize images to the ViT model's input size\n        ToTensor(),                       # Convert images to tensors\n        normalize                         # Normalize images using mean and std\n    ]\n)\n\n# Define a function to apply training transformations to a batch of examples\ndef train_transforms(examples):\n    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n    return examples\n\n# Define a function to apply validation transformations to a batch of examples\ndef val_transforms(examples):\n    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n    return examples","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:47.657640Z","iopub.execute_input":"2023-10-22T17:33:47.657992Z","iopub.status.idle":"2023-10-22T17:33:47.923195Z","shell.execute_reply.started":"2023-10-22T17:33:47.657964Z","shell.execute_reply":"2023-10-22T17:33:47.922220Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a5173472595428fb5f8c337b471a61f"}},"metadata":{}},{"name":"stdout","text":"Size:  224\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the transforms for the training data\ntrain_data.set_transform(train_transforms)\n\n# Set the transforms for the test/validation data\ntest_data.set_transform(val_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:48.915785Z","iopub.execute_input":"2023-10-22T17:33:48.916112Z","iopub.status.idle":"2023-10-22T17:33:48.931977Z","shell.execute_reply.started":"2023-10-22T17:33:48.916088Z","shell.execute_reply":"2023-10-22T17:33:48.931053Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Define a collate function that prepares batched data for model training.\ndef collate_fn(examples):\n    # Stack the pixel values from individual examples into a single tensor.\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    \n    # Convert the label strings in examples to corresponding numeric IDs using label2id dictionary.\n    labels = torch.tensor([example['label'] for example in examples])\n    \n    # Return a dictionary containing the batched pixel values and labels.\n    return {\"pixel_values\": pixel_values, \"labels\": labels}","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:49.818916Z","iopub.execute_input":"2023-10-22T17:33:49.819290Z","iopub.status.idle":"2023-10-22T17:33:49.825128Z","shell.execute_reply.started":"2023-10-22T17:33:49.819248Z","shell.execute_reply":"2023-10-22T17:33:49.824155Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Load, train, and evaluate model","metadata":{}},{"cell_type":"code","source":"# Create a ViTForImageClassification model from a pretrained checkpoint with a specified number of output labels.\nmodel = ViTForImageClassification.from_pretrained(model_str, num_labels=len(labels_list))\n\n# Configure the mapping of class labels to their corresponding indices for later reference.\nmodel.config.id2label = id2label\nmodel.config.label2id = label2id\n\n# Calculate and print the number of trainable parameters in millions for the model.\nprint(model.num_parameters(only_trainable=True) / 1e6)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:51.333133Z","iopub.execute_input":"2023-10-22T17:33:51.333908Z","iopub.status.idle":"2023-10-22T17:33:54.323770Z","shell.execute_reply.started":"2023-10-22T17:33:51.333872Z","shell.execute_reply":"2023-10-22T17:33:54.322892Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1534931b5f43ba9e5ca5b6d6b90cc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b677852f6b4f4640a9fa556748df5188"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"85.800194\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the accuracy metric from a module named 'evaluate'\naccuracy = evaluate.load(\"accuracy\")\n\n# Define a function 'compute_metrics' to calculate evaluation metrics\ndef compute_metrics(eval_pred):\n    # Extract model predictions from the evaluation prediction object\n    predictions = eval_pred.predictions\n    \n    # Extract true labels from the evaluation prediction object\n    label_ids = eval_pred.label_ids\n    \n    # Calculate accuracy using the loaded accuracy metric\n    # Convert model predictions to class labels by selecting the class with the highest probability (argmax)\n    predicted_labels = predictions.argmax(axis=1)\n    \n    # Calculate accuracy score by comparing predicted labels to true labels\n    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']\n    \n    # Return the computed accuracy as a dictionary with the key \"accuracy\"\n    return {\n        \"accuracy\": acc_score\n    }","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:54.473151Z","iopub.execute_input":"2023-10-22T17:33:54.473559Z","iopub.status.idle":"2023-10-22T17:33:55.182843Z","shell.execute_reply.started":"2023-10-22T17:33:54.473528Z","shell.execute_reply":"2023-10-22T17:33:55.182077Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c728ba003d8b4c56901705d7fb5954e9"}},"metadata":{}}]},{"cell_type":"code","source":"# Define the name of the evaluation metric to be used during training and evaluation.\nmetric_name = \"accuracy\"\n\n# Define the name of the model, which will be used to create a directory for saving model checkpoints and outputs.\nmodel_name = \"closed_eyes_image_detection\"\n\n# Define the number of training epochs for the model.\nnum_train_epochs = 2\n\n# Create an instance of TrainingArguments to configure training settings.\nargs = TrainingArguments(\n    # Specify the directory where model checkpoints and outputs will be saved.\n    output_dir=model_name,\n    \n    # Specify the directory where training logs will be stored.\n    logging_dir='./logs',\n    \n    # Define the evaluation strategy, which is performed at the end of each epoch.\n    evaluation_strategy=\"epoch\",\n    \n    # Set the learning rate for the optimizer.\n    learning_rate=1e-5,\n    \n    # Define the batch size for training on each device.\n    per_device_train_batch_size=64,\n    \n    # Define the batch size for evaluation on each device.\n    per_device_eval_batch_size=32,\n    \n    # Specify the total number of training epochs.\n    num_train_epochs=num_train_epochs,\n    \n    # Apply weight decay to prevent overfitting.\n    weight_decay=0.02,\n    \n    # Set the number of warm-up steps for the learning rate scheduler.\n    warmup_steps=50,\n    \n    # Disable the removal of unused columns from the dataset.\n    remove_unused_columns=False,\n    \n    # Define the strategy for saving model checkpoints (per epoch in this case).\n    save_strategy='epoch',\n    \n    # Load the best model at the end of training.\n    load_best_model_at_end=True,\n    \n    # Limit the total number of saved checkpoints to save space.\n    save_total_limit=1,\n    \n    # Specify that training progress should be reported to MLflow.\n    report_to=\"mlflow\"  # log to mlflow\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:55.805799Z","iopub.execute_input":"2023-10-22T17:33:55.806156Z","iopub.status.idle":"2023-10-22T17:33:55.843491Z","shell.execute_reply.started":"2023-10-22T17:33:55.806119Z","shell.execute_reply":"2023-10-22T17:33:55.842493Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Create a Trainer instance for fine-tuning a language model.\n\n# - `model`: The pre-trained language model to be fine-tuned.\n# - `args`: Configuration settings and hyperparameters for training.\n# - `train_dataset`: The dataset used for training the model.\n# - `eval_dataset`: The dataset used for evaluating the model during training.\n# - `data_collator`: A function that defines how data batches are collated and processed.\n# - `compute_metrics`: A function for computing custom evaluation metrics.\n# - `tokenizer`: The tokenizer used for processing text data.\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    tokenizer=processor,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:33:57.095673Z","iopub.execute_input":"2023-10-22T17:33:57.096421Z","iopub.status.idle":"2023-10-22T17:34:01.935605Z","shell.execute_reply.started":"2023-10-22T17:33:57.096387Z","shell.execute_reply":"2023-10-22T17:34:01.934766Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Evaluate the pre-training model's performance on a test dataset.\n# This function calculates various metrics such as accuracy, loss, etc.,\n# to assess how well the model is performing on unseen data.\n\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:34:01.937228Z","iopub.execute_input":"2023-10-22T17:34:01.937532Z","iopub.status.idle":"2023-10-22T17:37:18.105306Z","shell.execute_reply.started":"2023-10-22T17:34:01.937506Z","shell.execute_reply":"2023-10-22T17:37:18.104447Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1074' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [537/537 33:00]\n    </div>\n    "},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.7068123817443848,\n 'eval_accuracy': 0.3463127873814097,\n 'eval_runtime': 196.1188,\n 'eval_samples_per_second': 87.605,\n 'eval_steps_per_second': 2.738}"},"metadata":{}}]},{"cell_type":"code","source":"# Start training the model using the trainer object.\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T17:37:18.106388Z","iopub.execute_input":"2023-10-22T17:37:18.106674Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2113' max='2148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2113/2148 53:31 < 00:53, 0.66 it/s, Epoch 1.97/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.042700</td>\n      <td>0.043021</td>\n      <td>0.988243</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the post-training model's performance on the validation or test dataset.\n# This function computes various evaluation metrics like accuracy, loss, etc.\n# and provides insights into how well the model is performing.\n\ntrainer.evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the trained 'trainer' to make predictions on the 'test_data'.\noutputs = trainer.predict(test_data)\n\n# Print the metrics obtained from the prediction outputs.\nprint(outputs.metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the true labels from the model outputs\ny_true = outputs.label_ids\n\n# Predict the labels by selecting the class with the highest probability\ny_pred = outputs.predictions.argmax(1)\n\n# Define a function to plot a confusion matrix\ndef plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n    \"\"\"\n    This function plots a confusion matrix.\n\n    Parameters:\n        cm (array-like): Confusion matrix as returned by sklearn.metrics.confusion_matrix.\n        classes (list): List of class names, e.g., ['Class 0', 'Class 1'].\n        title (str): Title for the plot.\n        cmap (matplotlib colormap): Colormap for the plot.\n    \"\"\"\n    # Create a figure with a specified size\n    plt.figure(figsize=figsize)\n    \n    # Display the confusion matrix as an image with a colormap\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    # Define tick marks and labels for the classes on the axes\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.0f'\n    # Add text annotations to the plot indicating the values in the cells\n    thresh = cm.max() / 2.0\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Label the axes\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    # Ensure the plot layout is tight\n    plt.tight_layout()\n    # Display the plot\n    plt.show()\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred, average='macro')\n\n# Display accuracy and F1 score\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Get the confusion matrix if there are a small number of labels\nif len(labels_list) <= 150:\n    # Compute the confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    # Plot the confusion matrix using the defined function\n    plot_confusion_matrix(cm, labels_list, figsize=(8, 6))\n    \n# Finally, display classification report\nprint()\nprint(\"Classification report:\")\nprint()\nprint(classification_report(y_true, y_pred, target_names=labels_list, digits=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the trained model: This line of code is responsible for saving the model\n# that has been trained using the trainer object. It will serialize the model\n# and its associated weights, making it possible to reload and use the model\n# in the future without the need to retrain it.\ntrainer.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the 'pipeline' function from the 'transformers' library.\nfrom transformers import pipeline\n\n# Create a pipeline for image classification tasks. \n# You need to specify the 'model_name' and the 'device' to use for inference.\n# - 'model_name': The name of the pre-trained model to be used for image classification.\n# - 'device': Specifies the device to use for running the model (0 for GPU, -1 for CPU).\npipe = pipeline('image-classification', model=model_name, device=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accessing an image from the 'test_data' dataset using index 1.\nimage = test_data[1][\"image\"]\n\n# Displaying the 'image' variable.\nimage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the 'pipe' function to process the 'image' variable.\npipe(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This line of code accesses the \"label\" attribute of a specific element in the test_data list.\n# It's used to retrieve the actual label associated with a test data point.\nid2label[test_data[1][\"label\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Send model to Huggingface","metadata":{}},{"cell_type":"code","source":"# Import the necessary module to interact with the Hugging Face Hub.\nfrom huggingface_hub import notebook_login\n\n# Perform a login to the Hugging Face Hub.\nnotebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the HfApi class from the huggingface_hub library.\nfrom huggingface_hub import HfApi\n\n# Create an instance of the HfApi class.\napi = HfApi()\n\n# Define the repository ID by combining the username \"dima806\" with the model name.\nrepo_id = f\"dima806/{model_name}\"\n\ntry:\n    # Attempt to create a new repository on the Hugging Face Model Hub using the specified repo_id.\n    api.create_repo(repo_id)\n    \n    # If the repository creation is successful, print a message indicating that the repository was created.\n    print(f\"Repo {repo_id} created\")\nexcept:\n    # If an exception is raised, print a message indicating that the repository already exists.\n    print(f\"Repo {repo_id} already exists\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uploading a folder to the Hugging Face Model Hub\napi.upload_folder(\n    folder_path=model_name,  # The path to the folder to be uploaded\n    path_in_repo=\".\",  # The path where the folder will be stored in the repository\n    repo_id=repo_id,  # The ID of the repository where the folder will be uploaded\n    repo_type=\"model\",  # The type of the repository (in this case, a model repository)\n    revision=\"main\" # Revision name\n)","metadata":{},"execution_count":null,"outputs":[]}]}